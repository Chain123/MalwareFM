import numpy as np
from scipy import sparse
import json
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report
import sys
import os
from random import shuffle
import argparse
import math
# import pickle
from sklearn.externals import joblib
import timeit


parser = argparse.ArgumentParser()
parser.add_argument("-f", "--feature",
	                    help="sparse matrix stores embedded feature")
parser.add_argument("-l", "--label",									## this is only for non-sparse label vector case
	                    help="JSON file label of all samples")

args = parser.parse_args()
# if os.path.exists(args.feature) and os.path.exists(args.label):
loader1 = np.load(args.feature)
# loader2 = np.load("vector_label.npz")
feature = sparse.csr_matrix((loader1['data'],loader1['indices'],loader1['indptr']),shape=loader1['shape'])
label = json.load(open(args.label))

# svc=SVC()
label2 = np.array(label).reshape(len(label))
#       shuffle(label2)
# scl.set_params(C=10.0)
tuned_params=[{'kernel':['rbf','sigmoid'],'gamma': [5e-5,1e-5,5e-6],'C': [1,2,4],'class_weight': [{1:1}]}]

scores=['precision','recall']

# split trainning and testing data

# number_of_train = int(math.ceil(len(label)/2))?\
number_of_train = int(len(label2)*0.8)
number_of_test = len(label2) - number_of_train
start = 1000
train_feature = feature[0:number_of_train]
train_label = label2[0:number_of_train]

#test_feature = feature[start: number_of_test + start]
#test_label = label2[start: number_of_test + start]
test_feature = feature[number_of_train:]
test_label = label2[number_of_train: ]

# parameter selection

#	for score in scores:
#		print("parameter selecting with score %s ....." % score)
#		print(" ")
#		clf = GridSearchCV(SVC(),tuned_params,scoring = score)
#		clf.fit(train_feature,train_label)
#		print(" ")
#		print('Best params: ')
#		print(clf.best_params_)
#		print(" ")
	# print('results: ')
	# print(clf.cv_results_)
 
## training
if os.path.exists('../model/clf_p.pkl'):
    print('model exists, loading from ../model folder')
    clf_p = joblib.load('../model/clf_p.pkl')
    clf_r = joblib.load('../model/clf_r.pkl')
else:
    clf_p = SVC(kernel='rbf', C=1, gamma=1e-5, class_weight={1:1}, probability = True)
    clf_r = SVC(kernel='sigmoid', C=1, gamma=5e-5, class_weight={1:1}, probability = True)
    print("training....")
    clf_p.fit(train_feature,train_label)
    clf_r.fit(train_feature,train_label)
    ##  save model
    print('saving models')
    joblib.dump(clf_p,'../model/clf_p.pkl') 
    joblib.dump(clf_r,'../model/clf_r.pkl') 
    print(len(label))
print(len(train_label))
print(sum(test_label))
# # # ## prediction
# # # # prediction = np.array(clf.predict(test_feature))
   
prediction_p = list(np.array(clf_p.predict_proba(test_feature)))
prediction_r = list(np.array(clf_r.predict_proba(test_feature)))
print('saving test result')
pred_p = []
pred_r = []
for val in prediction_p:
    pred_p.append(val[1])
for val in prediction_r:
    pred_r.append(val[1])

final_result = zip(test_label, list(pred_p), list(pred_r))
with open('../result/test_result.json','w') as myfile:
    json.dump(final_result,myfile)    
# # # print(sum(prediction_p))

#	print('confusion matrix for the first set of params')
#	print(confusion_matrix(test_label,prediction_p))
#	print('confusion matrix for the second set of params')
#	print(confusion_matrix(test_label,prediction_r))
# errorfile = []
# allfilename = json.load(open("filename.json"))
# print(sum(label2))

# for ind, trueLabel in enumerate(label2):
# 	if trueLabel:
# 		if prediction[ind]:
# 			errorfile.append(allfilename[str(ind)])


# with open("errorfile.json",'w') as myfile:
# 	json.dump(errorfile,myfile,indent=4,sort_keys=True)
# prediction_r = np.array(clf_r.predict(test_feature))
# number_of_test = test_label.size + 0.0
# print("confusion matrix for the first set of params")
# print(confusion_matrix(test_label,prediction_p))
# print(" ")




# print("Accuracy"
# print("confusion matrix for the second set of params")
# print(confusion_matrix(test_label,prediction_r))

# print("Testing with " + str(number_of_test) + " samples")


# print("Best params: ")
# print(clf.best_params_)

# if prediction.size != test_label.size:
# 	print('ERROR!')
# else:
# 	error = (prediction==test_label)+0
# 	ERROR = sum(error)
# 	print("miss-classified: " + str(number_of_test - ERROR) + " samples")
# 	print("Accuracy: " + str(ERROR/number_of_test))
# 	print(confusion_matrix(test_label,prediction))

print("Done!")

